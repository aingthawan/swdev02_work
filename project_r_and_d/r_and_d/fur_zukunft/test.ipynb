{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6299e66a-cdca-4352-a187-3470d0e40438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File for class pageScraper\n",
    "# For scrape single page, Return dictionary URL, all backlinks and Raw Text\n",
    "# Input : url   ------>>  Output : {\n",
    "                                #     \"url\" : url,\n",
    "                                #     \"backlinks\" : self.scrape_all_urls(raw_soup_html),\n",
    "                                #     \"rawText\" : self.scrape_raw_text(raw_soup_html)\n",
    "                                # }\n",
    "# To Use : obj.scrape_page(url)\n",
    "# \n",
    "# dev : Aingkk.\n",
    "#                UML Diagram\n",
    "#                +-----------------------------------+\n",
    "#                | pageScraper                       |\n",
    "#                +-----------------------------------+\n",
    "#                | -allowed_domain: list             |\n",
    "#                +-----------------------------------+\n",
    "#                | +__init__()                       |\n",
    "#                | +get_raw_html(url: str)           |\n",
    "#                | +scrape_raw_text(soup_obj: bs4)   |\n",
    "#                | +scrape_all_urls(soup_obj: bs4)   |\n",
    "#                | +scrape_page(url: str)            |\n",
    "#                +-----------------------------------+\n",
    "\n",
    "import validators\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "\n",
    "class pageScraper:\n",
    "    \"\"\"Class for Scrape single page, Return dictionary URL, all backlinks and Raw Text\"\"\"\n",
    "    def __init__(self):\n",
    "        # Set allowed domain\n",
    "        self.allowed_domain = [\n",
    "            \"artyt.me\",\n",
    "            \"www.35mmc.com\",\n",
    "            \"www.dpreview.com\"\n",
    "        ]\n",
    "    \n",
    "    def get_raw_html(self, url):\n",
    "        \"\"\"get raw html soup obj\"\"\"\n",
    "        # webReq = requests.get(url)\n",
    "        return requests.get(url)\n",
    "    \n",
    "    def scrape_raw_text(self, html_text):\n",
    "        \"\"\"Return raw text string from bs4 boject\"\"\"\n",
    "        # return ' '.join([raw.text for raw in soup_obj.find_all(['h1', 'p'])])\n",
    "        soup = BeautifulSoup(html_text, 'html.parser')\n",
    "        return soup.get_text()\n",
    "    \n",
    "    def scrape_all_urls(self, html_text):\n",
    "        soup = BeautifulSoup(html_text, 'html.parser')\n",
    "        urls = []\n",
    "        for link in soup.find_all('a'):\n",
    "            url = link.get('href')\n",
    "            if url and re.match(\"^(http://|https://)\", url) and not re.search(\".(jpg|jpeg|png|gif)$\", url):\n",
    "                urls.append(url)\n",
    "        return list(set(urls))\n",
    "    \n",
    "    def scrape_page(self, url):\n",
    "        \"\"\"Return a dictionary of url, all unrepeated backlinks and raw text\"\"\"\n",
    "        raw_soup_html = self.get_raw_html(url).text\n",
    "        return {\n",
    "            \"url\" : url,\n",
    "            \"backlinks\" : self.scrape_all_urls(raw_soup_html),\n",
    "            \"rawText\" : self.scrape_raw_text(raw_soup_html)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "81ff489b-aea5-42ab-b22b-44e70fe0eed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = pageScraper()\n",
    "url = 'https://www.digitalcameraworld.com/news'\n",
    "ans = obj.scrape_page(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166104df-7bdf-4c5e-870c-a795ae65a1d8",
   "metadata": {},
   "source": [
    "print(ans['url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf167884-21a3-4282-ab6a-bd3af024ee4e",
   "metadata": {},
   "source": [
    "print(ans['backlinks'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279d23d7-b2bc-4907-9c5f-ebd9cd3e2cc1",
   "metadata": {},
   "source": [
    "print(ans['rawText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "631723f8-f576-48c7-b6bd-96d1d66a2b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ae1416a-717f-4e1e-8a74-14285c191314",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48f6bffa-feac-4cf7-b5fa-4b5b836b0095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x2977e9b66c0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute(\"CREATE TABLE IF NOT EXISTS Reference_Domain(Domain_Name, Ref_Count)\")\n",
    "cursor.execute(\"CREATE TABLE IF NOT EXISTS web_Data(Web_ID, URL, All_Word, Ref_To)\")\n",
    "cursor.execute(\"CREATE TABLE IF NOT EXISTS Inverted_Index(Word, Document_Freq, Inverted_Dict)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1891b9a3-104b-4cac-8a26-0a3addcab60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "class LinkChecker:\n",
    "    \"\"\"Class for working on URLs\"\"\"\n",
    "    \n",
    "    def __init__(self, database_file):\n",
    "        \"\"\"Input Database file\"\"\"\n",
    "        self.conn = sqlite3.connect(database_file)\n",
    "        self.cursor = self.conn.cursor()\n",
    "    \n",
    "    def alreadyScrape(self, url_to_check, table, column)\n",
    "        \"\"\"Check whether url already scrape, Return in True or false\n",
    "        Table : Reference_Domain,   \n",
    "        column_name : Domain_Name \"\"\"\n",
    "\n",
    "        query_check = f\"SELECT * FROM {table_name} WHERE {column_name}='{url_to_check}'\"\n",
    "        self.cursor.execute(query_check)\n",
    "        result = self.cursor.fetchone()\n",
    "\n",
    "        if result:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def checkAccessibility(self, url):\n",
    "        \"\"\"Check Whether URL is still accessible\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            return True\n",
    "        except requests.exceptions.HTTPError as err:\n",
    "            return False\n",
    "\n",
    "    def compareDomains(self, url1, url2):\n",
    "        \"\"\"Compare two url domain\"\"\"\n",
    "        domain1 = urlparse(url1).hostname\n",
    "        domain2 = urlparse(url2).hostname\n",
    "        return domain1 == domain2\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71dbb0c-972a-47e2-9acf-df48648cd144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "class dataPipeline:\n",
    "    \"\"\"Class of function for Update / Remove data\"\"\"\n",
    "    \n",
    "    def __init__(self, database_file):\n",
    "        \"\"\"Input database file\"\"\"\n",
    "        self.conn = sqlite3.connect(database_file)\n",
    "        self.cursor = self.conn.cursor()\n",
    "        self.createTable()\n",
    "        \n",
    "    def createTable(self):\n",
    "        # Create table for keeping domain name of url and times of referenced to\n",
    "        cursor.execute(\"CREATE TABLE IF NOT EXISTS Reference_Domain(Domain_Name, Ref_Count)\")\n",
    "        # Create a table for unique id for each url and list of all words in that url and list of url found on that page\n",
    "        cursor.execute(\"CREATE TABLE IF NOT EXISTS web_Data(Web_ID, URL, All_Word, Ref_To)\")\n",
    "        # Create table for each word, number of documnet that conatain that word and dictionary of sorted key that are id of url and number of that word found on that link\n",
    "        cursor.execute(\"CREATE TABLE IF NOT EXISTS Inverted_Index(Word, Document_Freq, Inverted_Dict)\")\n",
    "\n",
    "    def uncountRef(domain_name_list, tableName, domainColumn, countColumn):\n",
    "        \"\"\"For uncount referenced domain\"\"\"\n",
    "        for domain in domain_name_list:\n",
    "            query_check = f\"UPDATE {tableName} SET {countColumn} = {countColumn} - 1 WHERE {domainColumn} = '{domain}'\"\n",
    "            cursor.execute(query_check)\n",
    "            conn.commit()\n",
    "\n",
    "            \n",
    "    def removeInvertedIndex(table_name, doc_freq_col, inverted_dict_col, words, web_id):\n",
    "        \"\"\"Remove id from indexing and reduce docsfreq\"\"\"\n",
    "\n",
    "        for word in words:\n",
    "            # Retrieve the current values of Document_Freq and Inverted_Dict\n",
    "            self.cursor.execute(f\"SELECT {doc_freq_col}, {inverted_dict_col} FROM {table_name} WHERE Word=?\", (word,))\n",
    "            result = self.cursor.fetchone()\n",
    "            doc_freq, inverted_dict = result[0], result[1]\n",
    "\n",
    "            # Decrement the Document_Freq value\n",
    "            doc_freq -= 1\n",
    "\n",
    "            # Convert the Inverted_Dict string to a dictionary and remove the entry for the Web_ID\n",
    "            inverted_dict = eval(inverted_dict)\n",
    "            inverted_dict.pop(str(web_id), None)\n",
    "\n",
    "            # Update the values of Document_Freq and Inverted_Dict for the word\n",
    "            self.cursor.execute(f\"UPDATE {table_name} SET {doc_freq_col}=?, {inverted_dict_col}=? WHERE Word=?\", (doc_freq, str(inverted_dict), word))\n",
    "\n",
    "        # Commit the changes to the database\n",
    "        self.conn.commit()\n",
    "        \n",
    "        \n",
    "    def getUniqueID(self, table_name, web_id_column):\n",
    "        \"\"\"function for unique unused ID for a website\"\"\"\n",
    "        self.cursor.execute(f\"SELECT MAX({web_id_column}) FROM {table_name}\")\n",
    "        max_id = self.cursor.fetchone()[0]\n",
    "        next_id = 1 if max_id is None else max_id + 1\n",
    "        self.cursor.execute(f\"SELECT {web_id_column} FROM {table_name} WHERE {web_id_column} = {next_id}\")\n",
    "        while self.cursor.fetchone() is not None:\n",
    "            next_id += 1\n",
    "        return next_id\n",
    "    \n",
    "    \n",
    "    def updateReferenceDomain(self, table_name, domain_col, ref_col, domains):\n",
    "        \"\"\"Update reference domain receiving a list of domain\"\"\"\n",
    "        for domain in domains:\n",
    "            # Check if the domain already exists in the table\n",
    "            self.cursor.execute(f\"SELECT {ref_col} FROM {table_name} WHERE {domain_col}=?\", (domain,))\n",
    "            result = self.cursor.fetchone()\n",
    "            \n",
    "            if result:\n",
    "                # If the domain already exists, increment the Ref_Count by 1\n",
    "                ref_count = result[0] + 1\n",
    "                self.cursor.execute(f\"UPDATE {table_name} SET {ref_col}=? WHERE {domain_col}=?\", (ref_count, domain))\n",
    "            else:\n",
    "                # If the domain doesn't exist, insert a new entry with Ref_Count set to 1\n",
    "                self.cursor.execute(f\"INSERT INTO {table_name} ({domain_col}, {ref_col}) VALUES (?, 1)\", (domain,))\n",
    "        \n",
    "        # Commit the changes to the database\n",
    "        self.conn.commit()\n",
    "    \n",
    "    \n",
    "    def updateWebData(self, table_name, web_id_column, url_column, all_words_column, ref_to_column, url, web_id, words, domains):\n",
    "        \"\"\"Insert new url data into web_Data\"\"\"\n",
    "        all_words = \" \".join(words)\n",
    "        ref_to = \",\".join(domains)\n",
    "        \n",
    "        self.cursor.execute(f\"INSERT INTO {table_name} ({web_id_column}, {url_column}, {all_words_column}, {ref_to_column}) VALUES (?, ?, ?, ?)\", (web_id, url, all_words, ref_to))\n",
    "        self.conn.commit()\n",
    "        \n",
    "        \n",
    "    def updateInvertedIndexing(self, table_name, word_column, document_freq_column, inverted_dict_column, web_id, word_list):\n",
    "        word_count = {}\n",
    "        for word in word_list:\n",
    "            word_count[word] = word_count.get(word, 0) + 1\n",
    "        for word, count in word_count.items():\n",
    "            self.cursor.execute(f\"SELECT {word_column} FROM {table_name} WHERE {word_column} = '{word}'\")\n",
    "            result = self.cursor.fetchone()\n",
    "            if result:\n",
    "                self.cursor.execute(f\"UPDATE {table_name} SET {document_freq_column} = {document_freq_column} + 1, {inverted_dict_column} = {inverted_dict_column} || '{{{web_id}:{count}}}' WHERE {word_column} = '{word}'\")\n",
    "            else:\n",
    "                self.cursor.execute(f\"INSERT INTO {table_name} ({word_column}, {document_freq_column}, {inverted_dict_column}) VALUES ('{word}', 1, '{{{web_id}:{count}}}')\")\n",
    "        self.conn.commit()\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcab7f6-28c1-420d-9c8d-dd3aa37c2663",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://example.com/home\"\n",
    "LinkChecker = LinkChecker(\"test.db\")\n",
    "    \n",
    "if LinkChecker.alreadyScrape(url, \"Reference_Domain\", \"Domain_Name\"):\n",
    "    # Check if URL has been crawl\n",
    "    # If Yes\n",
    "    if LinkChecker.checkAccessibility(url):\n",
    "        pass\n",
    "    else:\n",
    "        # remove data\n",
    "    \n",
    "else:\n",
    "    # If Not\n",
    "    if LinkChecker.checkAccessibility(url):\n",
    "        # Insert New Data\n",
    "        \n",
    "    else:\n",
    "        pass\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ce36b53-0cc3-41fe-8a21-716ff3025939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Reference_Domain',), ('web_Data',), ('Inverted_Index',)]\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect(\"fur_zukunft/test.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create table for keeping domain name of url and times of referenced to\n",
    "cursor.execute(\"CREATE TABLE IF NOT EXISTS Reference_Domain(Domain_Name, Ref_Count)\")\n",
    "# Create a table for unique id for each url and list of all words in that url and list of url found on that page\n",
    "cursor.execute(\"CREATE TABLE IF NOT EXISTS web_Data(Web_ID, URL, All_Word, Ref_To)\")\n",
    "# Create table for each word, number of documnet that conatain that word and dictionary of sorted key that are id of url and number of that word found on that link\n",
    "cursor.execute(\"CREATE TABLE IF NOT EXISTS Inverted_Index(Word, Document_Freq, Inverted_Dict)\")\n",
    "\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1174bcd-b0d7-4ec9-9a85-10ea17452ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'example'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://example.com/home\"\n",
    "cursor.execute(\"SELECT All_Word FROM web_Data WHERE URL = ?\", (url,))\n",
    "domain_name = cursor.fetchone()\n",
    "domain_name[0].split(', ')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3d0bf06-46b5-4f72-bd0d-70ebdfe4c168",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = {\"hello\",\"world\"}\n",
    "domains = {\"www.google.com\",\"www.facebook.com\"}\n",
    "all_words = \" , \".join(words)\n",
    "ref_to = \" , \".join(domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24996cab-619e-49ca-af1a-9ed3a744ac12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('hello , world', 'www.google.com , www.facebook.com')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words, ref_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e098fa03-026b-4fd0-9626-f4f9c8071e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "class dataPipeline:\n",
    "    \"\"\"Class of function for Update / Remove data\"\"\"\n",
    "    \n",
    "    def __init__(self, database_file):\n",
    "        \"\"\"Input database file\"\"\"\n",
    "        self.conn = sqlite3.connect(database_file)\n",
    "        self.cursor = self.conn.cursor()\n",
    "        self.createTable()\n",
    "        \n",
    "    def createTable(self):\n",
    "        # Create table for keeping domain name of url and times of referenced to\n",
    "        cursor.execute(\"CREATE TABLE IF NOT EXISTS Reference_Domain(Domain_Name, Ref_Count)\")\n",
    "        # Create a table for unique id for each url and list of all words in that url and list of url found on that page\n",
    "        cursor.execute(\"CREATE TABLE IF NOT EXISTS web_Data(Web_ID, URL, All_Word, Ref_To)\")\n",
    "        # Create table for each word, number of documnet that conatain that word and dictionary of sorted key that are id of url and number of that word found on that link\n",
    "        cursor.execute(\"CREATE TABLE IF NOT EXISTS Inverted_Index(Word, Document_Freq, Inverted_Dict)\")\n",
    "\n",
    "    def uncountRef(self, domain_name_list):\n",
    "        \"\"\"For uncount referenced domain\"\"\"\n",
    "        for domain in domain_name_list:\n",
    "            query_check = f\"UPDATE Reference_Domain SET Ref_Count = Ref_Count - 1 WHERE Domain_Name = '{domain}'\"\n",
    "            cursor.execute(query_check)\n",
    "            conn.commit()\n",
    "\n",
    "            \n",
    "    def removeInvertedIndex(self, words, web_id):\n",
    "        \"\"\"Remove id from indexing and reduce docsfreq\"\"\"\n",
    "\n",
    "        for word in words:\n",
    "            # Retrieve the current values of Document_Freq and Inverted_Dict\n",
    "            self.cursor.execute(f\"SELECT Document_Freq, Inverted_Dict FROM Inverted_Index WHERE Word=?\", (word,))\n",
    "            result = self.cursor.fetchone()\n",
    "            doc_freq, inverted_dict = result[0], result[1]\n",
    "\n",
    "            # Decrement the Document_Freq value\n",
    "            doc_freq -= 1\n",
    "\n",
    "            # Convert the Inverted_Dict string to a dictionary and remove the entry for the Web_ID\n",
    "            inverted_dict = eval(inverted_dict)\n",
    "            inverted_dict.pop(str(web_id), None)\n",
    "\n",
    "            # Update the values of Document_Freq and Inverted_Dict for the word\n",
    "            self.cursor.execute(f\"UPDATE Inverted_Index SET Document_Freq=?, Inverted_Dict=? WHERE Word=?\", (doc_freq, str(inverted_dict), word))\n",
    "\n",
    "        # Commit the changes to the database\n",
    "        self.conn.commit()\n",
    "        \n",
    "        \n",
    "    def getUniqueID(self):\n",
    "        \"\"\"function for unique unused ID for a website\"\"\"\n",
    "        self.cursor.execute(f\"SELECT MAX(Web_ID) FROM web_Data\")\n",
    "        max_id = self.cursor.fetchone()[0]\n",
    "        next_id = 1 if max_id is None else max_id + 1\n",
    "        self.cursor.execute(f\"SELECT Web_ID FROM web_Data WHERE Web_ID = {next_id}\")\n",
    "        while self.cursor.fetchone() is not None:\n",
    "            next_id += 1\n",
    "        return next_id\n",
    "    \n",
    "    \n",
    "    # cursor.execute(\"CREATE TABLE IF NOT EXISTS Reference_Domain(Domain_Name, Ref_Count)\")\n",
    "    def updateReferenceDomain(self, domains):\n",
    "        \"\"\"Update reference domain receiving a list of domain\"\"\"\n",
    "        for domain in domains:\n",
    "            # Check if the domain already exists in the table\n",
    "            self.cursor.execute(f\"SELECT Ref_Count FROM Reference_Domain WHERE Domain_Name=?\", (domain,))\n",
    "            result = self.cursor.fetchone()\n",
    "            \n",
    "            if result:\n",
    "                # If the domain already exists, increment the Ref_Count by 1\n",
    "                ref_count = result[0] + 1\n",
    "                self.cursor.execute(f\"UPDATE Reference_Domain SET Ref_Count=? WHERE Domain_Name=?\", (ref_count, domain))\n",
    "            else:\n",
    "                # If the domain doesn't exist, insert a new entry with Ref_Count set to 1\n",
    "                self.cursor.execute(f\"INSERT INTO Reference_Domain (Domain_Name, Ref_Count) VALUES (?, 1)\", (domain,))\n",
    "        \n",
    "        # Commit the changes to the database\n",
    "        self.conn.commit()\n",
    "    \n",
    "    def updateWebData(self, web_id, url, all_words, ref_to):\n",
    "        \"\"\"Insert new url data into web_Data\"\"\"\n",
    "        all_words = \" \".join(words)\n",
    "        ref_to = \",\".join(domains)\n",
    "        \n",
    "        self.cursor.execute(f\"INSERT INTO web_Data (Web_ID, URL, All_Word, Ref_To) VALUES (?, ?, ?, ?)\", (web_id, url, all_words, ref_to))\n",
    "        self.conn.commit()\n",
    "        \n",
    "    \n",
    "    # cursor.execute(\"CREATE TABLE IF NOT EXISTS Inverted_Index(Word, Document_Freq, Inverted_Dict)\")\n",
    "    def updateInvertedIndexing(self, web_id, word_list):\n",
    "        word_count = {}\n",
    "        for word in word_list:\n",
    "            word_count[word] = word_count.get(word, 0) + 1\n",
    "        for word, count in word_count.items():\n",
    "            self.cursor.execute(f\"SELECT Word FROM Inverted_Index WHERE Word = '{word}'\")\n",
    "            result = self.cursor.fetchone()\n",
    "            if result:\n",
    "                self.cursor.execute(f\"UPDATE Inverted_Index SET Document_Freq = Document_Freq + 1, Inverted_Dict = Inverted_Dict || '{{{web_id}:{count}}}' WHERE Word = '{word}'\")\n",
    "            else:\n",
    "                self.cursor.execute(f\"INSERT INTO Inverted_Index (Word, Document_Freq, Inverted_Dict) VALUES ('{word}', 1, '{{{web_id}:{count}}}')\")\n",
    "        self.conn.commit()\n",
    "        \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
