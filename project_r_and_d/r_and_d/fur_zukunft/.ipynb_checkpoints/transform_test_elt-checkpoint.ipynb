{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c750048a-5a3c-48e8-87f7-f78a6fc57099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File for LinkChecker \n",
    "# For checking URL accessibility, compare URL domain, check whether URL already scrape\n",
    "# Dev : Aingkk\n",
    "\n",
    "import sqlite3\n",
    "from urllib.parse import urlparse\n",
    "import requests\n",
    "\n",
    "class LinkCheckers:\n",
    "    \"\"\"Class for working on URLs\"\"\"\n",
    "    \n",
    "    def __init__(self, database_file):\n",
    "        \"\"\"Input Database file\"\"\"\n",
    "        self.conn = sqlite3.connect(database_file)\n",
    "        self.cursor = self.conn.cursor()\n",
    "    \n",
    "    def alreadyScrape(self, url_to_check):\n",
    "        \"\"\"Check whether url already scrape, Return in True or false\"\"\"\n",
    "\n",
    "        # query_check = f\"SELECT * FROM Web_Data WHERE URL='{url_to_check}'\"\n",
    "        self.cursor.execute(f\"SELECT * FROM Web_Data WHERE URL='{url_to_check}'\")\n",
    "        result = self.cursor.fetchone()\n",
    "\n",
    "        if result:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    # def checkAccessibility(self, url):\n",
    "    #     \"\"\"Check Whether URL is still accessible\"\"\"\n",
    "    #     try:\n",
    "    #         response = requests.get(url)\n",
    "    #         response.raise_for_status()\n",
    "    #         return True\n",
    "    #     except requests.exceptions.HTTPError as err:\n",
    "    #         return False\n",
    "\n",
    "    def checkAccessibility(self, url):\n",
    "        \"\"\"Check Whether URL is still accessible\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            return True\n",
    "        except (requests.exceptions.HTTPError, requests.exceptions.RequestException) as err:\n",
    "            return False\n",
    "\n",
    "\n",
    "    def compareDomains(self, url1, url2):\n",
    "        \"\"\"Compare two url domain\"\"\"\n",
    "        domain1 = urlparse(url1).hostname\n",
    "        domain2 = urlparse(url2).hostname\n",
    "        return domain1 == domain2\n",
    "    \n",
    "    # method for terminate the connection\n",
    "    def close(self):\n",
    "        \"\"\"Close the connection\"\"\"\n",
    "        # commit the changes\n",
    "        self.conn.commit()\n",
    "        self.conn.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dbe89c3-7b6d-42b2-b804-67e8d5ee3250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "class TextCleaners:\n",
    "    \"\"\"Designed for Inverted Indexing\"\"\"\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def normalize(self, raw_text):\n",
    "        \"\"\"Remove special characters and lowercase text\"\"\"\n",
    "        return re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \" \", raw_text.lower())\n",
    "\n",
    "    def remove_stopwords(self, raw_text):\n",
    "        \"\"\"Remove stopwords\"\"\"\n",
    "        words = [word for word in raw_text.split() if word not in self.stop_words]\n",
    "        return \" \".join(words)\n",
    "\n",
    "    def lemmatize(self, raw_text):\n",
    "        \"\"\"Perform lemmatization, return as a list of strings\"\"\"\n",
    "        doc = self.nlp(raw_text)\n",
    "        return [token.lemma_ for token in doc]\n",
    "\n",
    "    def clean(self, raw_text):\n",
    "        \"\"\"Clean text by normalizing, removing stopwords, and lemmatizing\"\"\"\n",
    "        raw_text = self.normalize(raw_text)\n",
    "        raw_text = self.remove_stopwords(raw_text)\n",
    "        return self.lemmatize(raw_text)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51aacc84-a33c-4235-b730-635b1e643fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import validators\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "\n",
    "class pageScrapers:\n",
    "    \"\"\"Class for Scrape single page, Return dictionary URL, all backlinks and Raw Text\"\"\"\n",
    "    def __init__(self):\n",
    "        # Set header\n",
    "        self.headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
    "        self.proxies = self.read_proxies_from_file(\"proxy_valid.txt\")\n",
    "    \n",
    "    def read_proxies_from_file(self, filename):\n",
    "        \"\"\"Read proxies from a file and return as a list\"\"\"\n",
    "        proxies = []\n",
    "        with open(filename, \"r\") as f:\n",
    "            for line in f:\n",
    "                proxies.append(line.strip())\n",
    "        return proxies\n",
    "    \n",
    "    def get_raw_html(self, url):\n",
    "        \"\"\"get raw html soup obj using a rotating proxy\"\"\"\n",
    "        for proxy in self.proxies:\n",
    "            try:\n",
    "                res_temp = requests.get(url, headers=self.headers, proxies={\"http\": proxy, \"https\": proxy})\n",
    "                if res_temp.status_code == 200:\n",
    "                    return res_temp\n",
    "            except:\n",
    "                # catch any exception and continue to next proxy\n",
    "                continue\n",
    "        return None\n",
    "    \n",
    "    def scrape_raw_text(self, html_text):\n",
    "        \"\"\"Return raw text string from bs4 boject\"\"\"\n",
    "        soup = BeautifulSoup(html_text, 'html.parser')\n",
    "        return soup.get_text()\n",
    "    \n",
    "    def scrape_all_urls(self, html_text):\n",
    "        soup = BeautifulSoup(html_text, 'html.parser')\n",
    "        urls = []\n",
    "        for link in soup.find_all('a'):\n",
    "            url = link.get('href')\n",
    "            if url and re.match(\"^(http://|https://)\", url) and not re.search(\".(jpg|jpeg|png|gif)$\", url):\n",
    "                urls.append(url)\n",
    "        return list(set(urls))\n",
    "    \n",
    "    def scrape_page(self, url):\n",
    "        \"\"\"Return a dictionary of url, all unrepeated backlinks and raw text\"\"\"\n",
    "        raw_soup_html = self.get_raw_html(url).text\n",
    "        return {\n",
    "            \"url\" : url,\n",
    "            \"backlinks\" : self.scrape_all_urls(raw_soup_html),\n",
    "            \"rawText\" : self.scrape_raw_text(raw_soup_html)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56c5e83e-3533-4845-96e1-5df9b37695d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import ast\n",
    "\n",
    "class dataPipelines:\n",
    "    \"\"\"Class of function for Update / Remove data\"\"\"\n",
    "    \n",
    "    def __init__(self, database_file):\n",
    "        \"\"\"Input database file\"\"\"\n",
    "        self.conn = sqlite3.connect(database_file)\n",
    "        self.cursor = self.conn.cursor()\n",
    "        self.createTable()\n",
    "        \n",
    "    def createTable(self):\n",
    "        # Create table for keeping domain name of url and times of referenced to\n",
    "        self.cursor.execute(\"CREATE TABLE IF NOT EXISTS Reference_Domain(Domain_Name, Ref_Count)\")\n",
    "        # Create a table for unique id for each url and list of all words in that url and list of url found on that page\n",
    "        self.cursor.execute(\"CREATE TABLE IF NOT EXISTS web_Data(Web_ID, URL, All_Word, Ref_To)\")\n",
    "        # Create table for each word, number of documnet that conatain that word and dictionary of sorted key that are id of url and number of that word found on that link\n",
    "        self.cursor.execute(\"CREATE TABLE IF NOT EXISTS Inverted_Index(Word, Document_Freq, Inverted_Dict)\")\n",
    "\n",
    "    def uncountRef(self, domain_name_list):\n",
    "        \"\"\"For uncount referenced domain\"\"\"\n",
    "        for domain in domain_name_list:\n",
    "            # query_check = f\"UPDATE Reference_Domain SET Ref_Count = Ref_Count - 1 WHERE Domain_Name = '{domain}'\"\n",
    "            self.cursor.execute(f\"UPDATE Reference_Domain SET Ref_Count = Ref_Count - 1 WHERE Domain_Name = '{domain}'\")\n",
    "            self.conn.commit()\n",
    "\n",
    "            \n",
    "#     def removeInvertedIndex(self, web_id, words):\n",
    "#         \"\"\"Remove id from indexing and reduce docsfreq\"\"\"\n",
    "\n",
    "#         for word in words:\n",
    "#             # Retrieve the current values of Document_Freq and Inverted_Dict\n",
    "#             self.cursor.execute(f\"SELECT Document_Freq, Inverted_Dict FROM Inverted_Index WHERE Word=?\", (word,))\n",
    "#             result = self.cursor.fetchone()\n",
    "#             doc_freq, inverted_dict = result[0], result[1]\n",
    "\n",
    "#             # Decrement the Document_Freq value\n",
    "#             doc_freq -= 1\n",
    "\n",
    "#             # Convert the Inverted_Dict string to a dictionary and remove the entry for the Web_ID\n",
    "#             inverted_dict = eval(inverted_dict)\n",
    "#             inverted_dict.pop(str(web_id), None)\n",
    "\n",
    "#             # Update the values of Document_Freq and Inverted_Dict for the word\n",
    "#             self.cursor.execute(f\"UPDATE Inverted_Index SET Document_Freq=?, Inverted_Dict=? WHERE Word=?\", (doc_freq, str(inverted_dict), word))\n",
    "\n",
    "#         # Commit the changes to the database\n",
    "#         self.conn.commit()\n",
    "    \n",
    "    def removeInvertedIndex(self, web_id, words):\n",
    "        \"\"\"Remove id from indexing and reduce docsfreq\"\"\"\n",
    "        for word in words:\n",
    "            self.cursor.execute(\"SELECT Inverted_Dict FROM Inverted_Index WHERE Word=?\", (word,))\n",
    "            inverted_dict = eval(self.cursor.fetchone()[0])\n",
    "            inverted_dict.pop(web_id, None)\n",
    "            self.cursor.execute(f\"UPDATE Inverted_Index SET Document_Freq=Document_Freq-1, Inverted_Dict=? WHERE Word=?\", (str(inverted_dict), word))\n",
    "        self.conn.commit()\n",
    "\n",
    "        \n",
    "    def removeWebData(self, url):\n",
    "        \"\"\"Remove data from web_Data\"\"\"\n",
    "        self.cursor.execute(f\"DELETE FROM web_Data WHERE URL=?\", (url,))\n",
    "        self.conn.commit()\n",
    "\n",
    "    # ==============================================================\n",
    "\n",
    "\n",
    "    def getUniqueID(self):\n",
    "        \"\"\"function for unique unused ID for a website\"\"\"\n",
    "        self.cursor.execute(f\"SELECT MAX(Web_ID) FROM web_Data\")\n",
    "        max_id = self.cursor.fetchone()[0]\n",
    "        next_id = 1 if max_id is None else max_id + 1\n",
    "        self.cursor.execute(f\"SELECT Web_ID FROM web_Data WHERE Web_ID = {next_id}\")\n",
    "        while self.cursor.fetchone() is not None:\n",
    "            next_id += 1\n",
    "        return next_id\n",
    "    \n",
    "    def fetch_data_by_url(self, url):\n",
    "        \"\"\"get data from row by url\"\"\"\n",
    "        self.cursor.execute(\"SELECT Web_ID, URL, All_Word, Ref_To FROM web_Data WHERE URL=?\", (url,))\n",
    "        # Fetch the result\n",
    "        result = self.cursor.fetchone()\n",
    "        # Return the result\n",
    "        return {\n",
    "            'Web_ID' : result[0],\n",
    "            'URL' : result[1],\n",
    "            'All_Word' : result[2].split(' , '),\n",
    "            'Ref_To' : result[3].split(' , ')\n",
    "        }\n",
    "\n",
    "    # ==============================================================\n",
    "\n",
    "#     OKAY ================================================================================    \n",
    "    # cursor.execute(\"CREATE TABLE IF NOT EXISTS Reference_Domain(Domain_Name, Ref_Count)\")\n",
    "    def updateReferenceDomain(self, domains):\n",
    "        \"\"\"Update reference domain receiving a list of domain\"\"\"\n",
    "        for domain in domains:\n",
    "            # Check if the domain already exists in the table\n",
    "            self.cursor.execute(f\"SELECT Ref_Count FROM Reference_Domain WHERE Domain_Name=?\", (domain,))\n",
    "            result = self.cursor.fetchone()\n",
    "            \n",
    "            if result:\n",
    "                # If the domain already exists, increment the Ref_Count by 1\n",
    "                ref_count = result[0] + 1\n",
    "                self.cursor.execute(f\"UPDATE Reference_Domain SET Ref_Count=? WHERE Domain_Name=?\", (ref_count, domain))\n",
    "            else:\n",
    "                # If the domain doesn't exist, insert a new entry with Ref_Count set to 1\n",
    "                self.cursor.execute(f\"INSERT INTO Reference_Domain (Domain_Name, Ref_Count) VALUES (?, 1)\", (domain,))\n",
    "        \n",
    "        # Commit the changes to the database\n",
    "        self.conn.commit()\n",
    "    \n",
    "#     OKAY ================================================================================\n",
    "    def updateWebData(self, web_id, url, all_words, ref_to):\n",
    "        \"\"\"Insert new url data into web_Data\"\"\"\n",
    "        words = list(all_words.keys())\n",
    "        all_words = \" , \".join(words)\n",
    "        ref_to = \" , \".join(ref_to)\n",
    "        \n",
    "        self.cursor.execute(f\"INSERT INTO web_Data (Web_ID, URL, All_Word, Ref_To) VALUES (?, ?, ?, ?)\", (web_id, url, all_words, ref_to))\n",
    "        self.conn.commit()\n",
    "        \n",
    "    \n",
    "    # cursor.execute(\"CREATE TABLE IF NOT EXISTS Inverted_Index(Word, Document_Freq, Inverted_Dict)\")\n",
    "    def updateInvertedIndexing(self, web_id, word_list):\n",
    "        word_count = {}\n",
    "        for word in word_list:\n",
    "            word_count[word] = word_count.get(word, 0) + 1\n",
    "        for word, count in word_count.items():\n",
    "            self.cursor.execute(f\"SELECT Word, Inverted_Dict FROM Inverted_Index WHERE Word = '{word}'\")\n",
    "            result = self.cursor.fetchone()\n",
    "            if result:\n",
    "                inverted_dict = eval(result[1])\n",
    "                inverted_dict[web_id] = count\n",
    "                inverted_dict = str(inverted_dict)\n",
    "                self.cursor.execute(f\"UPDATE Inverted_Index SET Document_Freq = Document_Freq + 1, Inverted_Dict = '{inverted_dict}' WHERE Word = '{word}'\")\n",
    "            else:\n",
    "                self.cursor.execute(f\"INSERT INTO Inverted_Index (Word, Document_Freq, Inverted_Dict) VALUES ('{word}', 1, '{{{web_id}:{count}}}')\")\n",
    "        self.conn.commit()\n",
    "\n",
    "    \n",
    "    # def updateInvertedIndexing(self, web_id, word_list):\n",
    "    #     word_count = {}\n",
    "    #     for word in word_list:\n",
    "    #         word_count[word] = word_count.get(word, 0) + 1\n",
    "    #     for word, count in word_count.items():\n",
    "    #         self.cursor.execute(f\"SELECT Word FROM Inverted_Index WHERE Word = '{word}'\")\n",
    "    #         result = self.cursor.fetchone()\n",
    "    #         if result:\n",
    "    #             self.cursor.execute(f\"UPDATE Inverted_Index SET Document_Freq = Document_Freq + 1, Inverted_Dict = Inverted_Dict || '{','.join([f'{{{web_id}:{count}}}' for web_id, count in word_count.items()])}' WHERE Word = '{word}'\")\n",
    "    #         else:\n",
    "    #             self.cursor.execute(f\"INSERT INTO Inverted_Index (Word, Document_Freq, Inverted_Dict) VALUES ('{word}', 1, '{','.join([f'{{{web_id}:{count}}}' for web_id, count in word_count.items()])}')\")\n",
    "    #     self.conn.commit()\n",
    "\n",
    "\n",
    "        \n",
    "    # method for terminate the connection\n",
    "    def close(self):\n",
    "        \"\"\"Close the connection\"\"\"\n",
    "        # commit the changes\n",
    "        self.conn.commit()\n",
    "        self.conn.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "956abd87-2cca-4c88-9368-9a105f4500d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from linkChecker import *\n",
    "# from dataPipeline import *\n",
    "# from cleanRawText import *\n",
    "# from singleScrape import *\n",
    "\n",
    "import sqlite3\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# ================================================================================================\n",
    "# ================================================================================================\n",
    "\n",
    "def word_frequency_dict(words_list):\n",
    "    \"\"\"Turn list of words into dictionary with word as key and frequency as value\"\"\"\n",
    "    frequency_dict = {}\n",
    "    for word in words_list:\n",
    "        if word in frequency_dict:\n",
    "            frequency_dict[word] += 1\n",
    "        else:\n",
    "            frequency_dict[word] = 1\n",
    "    return frequency_dict    \n",
    "\n",
    "# ================================================================================================\n",
    "# ================================================================================================\n",
    "\n",
    "class raw_database:\n",
    "    \"\"\"class for getting the raw content from the database and remove\"\"\"\n",
    "\n",
    "    def __init__(self, database):\n",
    "        \"\"\"initialize the database\"\"\"\n",
    "        self.conn = sqlite3.connect(database)\n",
    "        self.cur = self.conn.cursor()\n",
    "\n",
    "    def get_row(self):\n",
    "        \"\"\"get the row from the database\"\"\"\n",
    "        self.cur.execute(\"SELECT * FROM rawMaterial LIMIT 1\")\n",
    "        row = self.cur.fetchone()\n",
    "        \n",
    "        if row is None:\n",
    "            return None\n",
    "        else:\n",
    "            return row\n",
    "\n",
    "    def delete_row(self, url):\n",
    "        \"\"\"delete the row from the database\"\"\"\n",
    "        self.cur.execute(\"DELETE FROM rawMaterial WHERE url = ?\", (url,))\n",
    "        self.conn.commit()\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"close the connection\"\"\"\n",
    "        self.conn.commit()\n",
    "        self.conn.close()\n",
    "\n",
    "\n",
    "\n",
    "class main_database:\n",
    "    \"\"\"class for processing the raw content and insert into the database\"\"\"\n",
    "\n",
    "    def __init__(self, main_database):\n",
    "        \"\"\"initialize the database\"\"\"\n",
    "        self.tc = TextCleaners()\n",
    "        self.ps = pageScrapers()\n",
    "        self.dp = dataPipelines(main_database)\n",
    "\n",
    "    def get_domain(self, url):\n",
    "        \"\"\"Get domain name (example.com) from a url\"\"\"\n",
    "        parsed_url = urlparse(url)\n",
    "        domain = parsed_url.netloc\n",
    "        if domain.startswith('www.'):\n",
    "            domain = domain[4:]\n",
    "        return domain\n",
    "\n",
    "    def updateLink(self, url, raw_content):\n",
    "        \"\"\"update the link into the database\"\"\"\n",
    "        # Clean raw content\n",
    "        clean_content = self.tc.clean(raw_content)\n",
    "        clean_content_dict = self.word_frequency_dict(clean_content)\n",
    "        \n",
    "        all_backlink_list = self.ps.scrape_all_urls(raw_content)\n",
    "        \n",
    "        url_domain = self.get_domain(url)\n",
    "        # get a list of unique domain from all backlinks\n",
    "        all_backlink_domain_list = []\n",
    "        for link in all_backlink_list:\n",
    "            link_domain = self.get_domain(link)\n",
    "            if (self.get_domain(link) not in all_backlink_domain_list) and (link_domain != url_domain):\n",
    "                all_backlink_domain_list.append(link_domain)\n",
    "        \n",
    "        new_id = self.dp.getUniqueID()\n",
    "    \n",
    "        # updating reference domain\n",
    "        self.dp.updateReferenceDomain(all_backlink_domain_list)\n",
    "        # update webData\n",
    "        # def updateWebData(self, web_id, url, all_words, ref_to):\n",
    "        self.dp.updateWebData(new_id, url, clean_content_dict, all_backlink_domain_list)\n",
    "        # update invertedIndex\n",
    "        self.dp.updateInvertedIndexing(new_id, clean_content)\n",
    "\n",
    "\n",
    "    def removeData(self, url):\n",
    "        \"\"\"remove the data from the database\"\"\"\n",
    "        temp_datarow = self.dp.fetch_data_by_url(url)\n",
    "        self.dp.removeWebData(temp_datarow['URL'])\n",
    "        self.dp.uncountRef(temp_datarow['Ref_To'])\n",
    "        self.dp.removeInvertedIndex(temp_datarow['Web_ID'], temp_datarow['All_Word'])\n",
    "        \n",
    "    def word_frequency_dict(self, words_list):\n",
    "        \"\"\"Turn list of words into dictionary with word as key and frequency as value\"\"\"\n",
    "        frequency_dict = {}\n",
    "        for word in words_list:\n",
    "            if word in frequency_dict:\n",
    "                frequency_dict[word] += 1\n",
    "            else:\n",
    "                frequency_dict[word] = 1\n",
    "        return frequency_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac00130-106b-4d9e-9d50-38aa88df3c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table is empty. Waiting for data...\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import time\n",
    "\n",
    "def data_processing():\n",
    "    \n",
    "    directory = \"\"\n",
    "    raw_dir = directory + \"database_elt_raw_test3.db\"\n",
    "    main_dir = directory + \"database_elt_test1.db\"\n",
    "    \n",
    "    rawd = raw_database(raw_dir)\n",
    "    mdb = main_database(main_dir)\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        row_temp = rawd.get_row()\n",
    "        \n",
    "        if row_temp is None:\n",
    "            print(\"Table is empty. Waiting for data...\")\n",
    "            time.sleep(5)\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            url = row_temp[0]\n",
    "            raw = row_temp[1]\n",
    "            mdb.updateLink(url, raw)\n",
    "            rawd.delete_row(url)\n",
    "        \n",
    "        \n",
    "if __name__ == \"__main__\":    \n",
    "    try:\n",
    "        data_processing()\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Exiting program\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d99fd0-dc74-48d4-8777-0be5f66c6fca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
