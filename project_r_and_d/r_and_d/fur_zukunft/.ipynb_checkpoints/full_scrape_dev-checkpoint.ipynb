{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fddeb0b-3609-4f16-bc0f-8c2ed3df8253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "class TextCleaners:\n",
    "    \"\"\"Designed for Inverted Indexing\"\"\"\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def normalize(self, raw_text):\n",
    "        \"\"\"Remove special characters and lowercase text\"\"\"\n",
    "        return re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \" \", raw_text.lower())\n",
    "\n",
    "    def remove_stopwords(self, raw_text):\n",
    "        \"\"\"Remove stopwords\"\"\"\n",
    "        words = [word for word in raw_text.split() if word not in self.stop_words]\n",
    "        return \" \".join(words)\n",
    "\n",
    "    def lemmatize(self, raw_text):\n",
    "        \"\"\"Perform lemmatization, return as a list of strings\"\"\"\n",
    "        doc = self.nlp(raw_text)\n",
    "        return [token.lemma_ for token in doc]\n",
    "\n",
    "    def clean(self, raw_text):\n",
    "        \"\"\"Clean text by normalizing, removing stopwords, and lemmatizing\"\"\"\n",
    "        raw_text = self.normalize(raw_text)\n",
    "        raw_text = self.remove_stopwords(raw_text)\n",
    "        return self.lemmatize(raw_text)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d1e99eb-7912-4b02-80b6-b3cf55695fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "class dataPipelines:\n",
    "    \"\"\"Class of function for Update / Remove data\"\"\"\n",
    "    \n",
    "    def __init__(self, database_file):\n",
    "        \"\"\"Input database file\"\"\"\n",
    "        self.conn = sqlite3.connect(database_file)\n",
    "        self.cursor = self.conn.cursor()\n",
    "        self.createTable()\n",
    "        \n",
    "    def createTable(self):\n",
    "        # Create table for keeping domain name of url and times of referenced to\n",
    "        self.cursor.execute(\"CREATE TABLE IF NOT EXISTS Reference_Domain(Domain_Name, Ref_Count)\")\n",
    "        # Create a table for unique id for each url and list of all words in that url and list of url found on that page\n",
    "        self.cursor.execute(\"CREATE TABLE IF NOT EXISTS web_Data(Web_ID, URL, All_Word, Ref_To)\")\n",
    "        # Create table for each word, number of documnet that conatain that word and dictionary of sorted key that are id of url and number of that word found on that link\n",
    "        self.cursor.execute(\"CREATE TABLE IF NOT EXISTS Inverted_Index(Word, Document_Freq, Inverted_Dict)\")\n",
    "\n",
    "    def uncountRef(self, domain_name_list):\n",
    "        \"\"\"For uncount referenced domain\"\"\"\n",
    "        for domain in domain_name_list:\n",
    "            # query_check = f\"UPDATE Reference_Domain SET Ref_Count = Ref_Count - 1 WHERE Domain_Name = '{domain}'\"\n",
    "            self.cursor.execute(f\"UPDATE Reference_Domain SET Ref_Count = Ref_Count - 1 WHERE Domain_Name = '{domain}'\")\n",
    "            self.conn.commit()\n",
    "\n",
    "            \n",
    "#     def removeInvertedIndex(self, web_id, words):\n",
    "#         \"\"\"Remove id from indexing and reduce docsfreq\"\"\"\n",
    "\n",
    "#         for word in words:\n",
    "#             # Retrieve the current values of Document_Freq and Inverted_Dict\n",
    "#             self.cursor.execute(f\"SELECT Document_Freq, Inverted_Dict FROM Inverted_Index WHERE Word=?\", (word,))\n",
    "#             result = self.cursor.fetchone()\n",
    "#             doc_freq, inverted_dict = result[0], result[1]\n",
    "\n",
    "#             # Decrement the Document_Freq value\n",
    "#             doc_freq -= 1\n",
    "\n",
    "#             # Convert the Inverted_Dict string to a dictionary and remove the entry for the Web_ID\n",
    "#             inverted_dict = eval(inverted_dict)\n",
    "#             inverted_dict.pop(str(web_id), None)\n",
    "\n",
    "#             # Update the values of Document_Freq and Inverted_Dict for the word\n",
    "#             self.cursor.execute(f\"UPDATE Inverted_Index SET Document_Freq=?, Inverted_Dict=? WHERE Word=?\", (doc_freq, str(inverted_dict), word))\n",
    "\n",
    "#         # Commit the changes to the database\n",
    "#         self.conn.commit()\n",
    "    \n",
    "    def removeInvertedIndex(self, web_id, words):\n",
    "        \"\"\"Remove id from indexing and reduce docsfreq\"\"\"\n",
    "        for word in words:\n",
    "            self.cursor.execute(\"SELECT Inverted_Dict FROM Inverted_Index WHERE Word=?\", (word,))\n",
    "            inverted_dict = eval(self.cursor.fetchone()[0])\n",
    "            inverted_dict.pop(web_id, None)\n",
    "            self.cursor.execute(f\"UPDATE Inverted_Index SET Document_Freq=Document_Freq-1, Inverted_Dict=? WHERE Word=?\", (str(inverted_dict), word))\n",
    "        self.conn.commit()\n",
    "\n",
    "        \n",
    "    def removeWebData(self, url):\n",
    "        \"\"\"Remove data from web_Data\"\"\"\n",
    "        self.cursor.execute(f\"DELETE FROM web_Data WHERE URL=?\", (url,))\n",
    "        self.conn.commit()\n",
    "\n",
    "    # ==============================================================\n",
    "\n",
    "\n",
    "    def getUniqueID(self):\n",
    "        \"\"\"function for unique unused ID for a website\"\"\"\n",
    "        self.cursor.execute(f\"SELECT MAX(Web_ID) FROM web_Data\")\n",
    "        max_id = self.cursor.fetchone()[0]\n",
    "        next_id = 1 if max_id is None else max_id + 1\n",
    "        self.cursor.execute(f\"SELECT Web_ID FROM web_Data WHERE Web_ID = {next_id}\")\n",
    "        while self.cursor.fetchone() is not None:\n",
    "            next_id += 1\n",
    "        return next_id\n",
    "    \n",
    "    def fetch_data_by_url(self, url):\n",
    "        \"\"\"get data from row by url\"\"\"\n",
    "        self.cursor.execute(\"SELECT Web_ID, URL, All_Word, Ref_To FROM web_Data WHERE URL=?\", (url,))\n",
    "        # Fetch the result\n",
    "        result = self.cursor.fetchone()\n",
    "        # Return the result\n",
    "        return {\n",
    "            'Web_ID' : result[0],\n",
    "            'URL' : result[1],\n",
    "            'All_Word' : result[2].split(' , '),\n",
    "            'Ref_To' : result[3].split(' , ')\n",
    "        }\n",
    "\n",
    "    # ==============================================================\n",
    "\n",
    "#     OKAY ================================================================================    \n",
    "    # cursor.execute(\"CREATE TABLE IF NOT EXISTS Reference_Domain(Domain_Name, Ref_Count)\")\n",
    "    def updateReferenceDomain(self, domains):\n",
    "        \"\"\"Update reference domain receiving a list of domain\"\"\"\n",
    "        for domain in domains:\n",
    "            # Check if the domain already exists in the table\n",
    "            self.cursor.execute(f\"SELECT Ref_Count FROM Reference_Domain WHERE Domain_Name=?\", (domain,))\n",
    "            result = self.cursor.fetchone()\n",
    "            \n",
    "            if result:\n",
    "                # If the domain already exists, increment the Ref_Count by 1\n",
    "                ref_count = result[0] + 1\n",
    "                self.cursor.execute(f\"UPDATE Reference_Domain SET Ref_Count=? WHERE Domain_Name=?\", (ref_count, domain))\n",
    "            else:\n",
    "                # If the domain doesn't exist, insert a new entry with Ref_Count set to 1\n",
    "                self.cursor.execute(f\"INSERT INTO Reference_Domain (Domain_Name, Ref_Count) VALUES (?, 1)\", (domain,))\n",
    "        \n",
    "        # Commit the changes to the database\n",
    "        self.conn.commit()\n",
    "    \n",
    "#     OKAY ================================================================================\n",
    "    def updateWebData(self, web_id, url, all_words, ref_to):\n",
    "        \"\"\"Insert new url data into web_Data\"\"\"\n",
    "        words = list(all_words.keys())\n",
    "        all_words = \" , \".join(words)\n",
    "        ref_to = \" , \".join(ref_to)\n",
    "        \n",
    "        self.cursor.execute(f\"INSERT INTO web_Data (Web_ID, URL, All_Word, Ref_To) VALUES (?, ?, ?, ?)\", (web_id, url, all_words, ref_to))\n",
    "        self.conn.commit()\n",
    "        \n",
    "    \n",
    "    # cursor.execute(\"CREATE TABLE IF NOT EXISTS Inverted_Index(Word, Document_Freq, Inverted_Dict)\")\n",
    "    def updateInvertedIndexing(self, web_id, word_list):\n",
    "        word_count = {}\n",
    "        for word in word_list:\n",
    "            word_count[word] = word_count.get(word, 0) + 1\n",
    "        for word, count in word_count.items():\n",
    "            self.cursor.execute(f\"SELECT Word, Inverted_Dict FROM Inverted_Index WHERE Word = '{word}'\")\n",
    "            result = self.cursor.fetchone()\n",
    "            if result:\n",
    "                inverted_dict = eval(result[1])\n",
    "                inverted_dict[web_id] = count\n",
    "                inverted_dict = str(inverted_dict)\n",
    "                self.cursor.execute(f\"UPDATE Inverted_Index SET Document_Freq = Document_Freq + 1, Inverted_Dict = '{inverted_dict}' WHERE Word = '{word}'\")\n",
    "            else:\n",
    "                self.cursor.execute(f\"INSERT INTO Inverted_Index (Word, Document_Freq, Inverted_Dict) VALUES ('{word}', 1, '{{{web_id}:{count}}}')\")\n",
    "        self.conn.commit()\n",
    "\n",
    "    \n",
    "    # def updateInvertedIndexing(self, web_id, word_list):\n",
    "    #     word_count = {}\n",
    "    #     for word in word_list:\n",
    "    #         word_count[word] = word_count.get(word, 0) + 1\n",
    "    #     for word, count in word_count.items():\n",
    "    #         self.cursor.execute(f\"SELECT Word FROM Inverted_Index WHERE Word = '{word}'\")\n",
    "    #         result = self.cursor.fetchone()\n",
    "    #         if result:\n",
    "    #             self.cursor.execute(f\"UPDATE Inverted_Index SET Document_Freq = Document_Freq + 1, Inverted_Dict = Inverted_Dict || '{','.join([f'{{{web_id}:{count}}}' for web_id, count in word_count.items()])}' WHERE Word = '{word}'\")\n",
    "    #         else:\n",
    "    #             self.cursor.execute(f\"INSERT INTO Inverted_Index (Word, Document_Freq, Inverted_Dict) VALUES ('{word}', 1, '{','.join([f'{{{web_id}:{count}}}' for web_id, count in word_count.items()])}')\")\n",
    "    #     self.conn.commit()\n",
    "\n",
    "\n",
    "        \n",
    "    # method for terminate the connection\n",
    "    def close(self):\n",
    "        \"\"\"Close the connection\"\"\"\n",
    "        # commit the changes\n",
    "        self.conn.commit()\n",
    "        self.conn.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ac763ec-3593-4d6b-a375-adc7882adbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "class LinkCheckers:\n",
    "    \"\"\"Class for working on URLs\"\"\"\n",
    "    \n",
    "    def __init__(self, database_file):\n",
    "        \"\"\"Input Database file\"\"\"\n",
    "        self.conn = sqlite3.connect(database_file)\n",
    "        self.cursor = self.conn.cursor()\n",
    "    \n",
    "    def alreadyScrape(self, url_to_check):\n",
    "        \"\"\"Check whether url already scrape, Return in True or false\"\"\"\n",
    "\n",
    "        # query_check = f\"SELECT * FROM Web_Data WHERE URL='{url_to_check}'\"\n",
    "        self.cursor.execute(f\"SELECT * FROM Web_Data WHERE URL='{url_to_check}'\")\n",
    "        result = self.cursor.fetchone()\n",
    "\n",
    "        if result:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def checkAccessibility(self, url):\n",
    "        \"\"\"Check Whether URL is still accessible\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            return True\n",
    "        except requests.exceptions.HTTPError as err:\n",
    "            return False\n",
    "\n",
    "    def compareDomains(self, url1, url2):\n",
    "        \"\"\"Compare two url domain\"\"\"\n",
    "        domain1 = urlparse(url1).hostname\n",
    "        domain2 = urlparse(url2).hostname\n",
    "        return domain1 == domain2\n",
    "    \n",
    "    # method for terminate the connection\n",
    "    def close(self):\n",
    "        \"\"\"Close the connection\"\"\"\n",
    "        # commit the changes\n",
    "        self.conn.commit()\n",
    "        self.conn.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcfe8213-ee48-40b6-8c58-647cf8e5987c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import validators\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "\n",
    "class pageScrapers:\n",
    "    \"\"\"Class for Scrape single page, Return dictionary URL, all backlinks and Raw Text\"\"\"\n",
    "    def __init__(self):\n",
    "        # Set header\n",
    "        self.headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}        \n",
    "        # Set allowed domain \n",
    "        # self.allowed_domain = [\n",
    "        #     \"artyt.me\",\n",
    "        #     \"www.35mmc.com\",\n",
    "        #     \"www.dpreview.com\"\n",
    "        # ]\n",
    "    \n",
    "    def get_raw_html(self, url):\n",
    "        \"\"\"get raw html soup obj\"\"\"\n",
    "        # webReq = requests.get(url)\n",
    "        # return requests.get(url, headers=self.headers)\n",
    "        res_temp = requests.get(url, headers=self.headers)\n",
    "        if res_temp.status_code == 200:\n",
    "            return res_temp\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def scrape_raw_text(self, html_text):\n",
    "        \"\"\"Return raw text string from bs4 boject\"\"\"\n",
    "        # return ' '.join([raw.text for raw in soup_obj.find_all(['h1', 'p'])])\n",
    "        soup = BeautifulSoup(html_text, 'html.parser')\n",
    "        return soup.get_text()\n",
    "    \n",
    "    def scrape_all_urls(self, html_text):\n",
    "        soup = BeautifulSoup(html_text, 'html.parser')\n",
    "        urls = []\n",
    "        for link in soup.find_all('a'):\n",
    "            url = link.get('href')\n",
    "            if url and re.match(\"^(http://|https://)\", url) and not re.search(\".(jpg|jpeg|png|gif)$\", url):\n",
    "                urls.append(url)\n",
    "        return list(set(urls))\n",
    "    \n",
    "    def scrape_page(self, url):\n",
    "        \"\"\"Return a dictionary of url, all unrepeated backlinks and raw text\"\"\"\n",
    "        raw_soup_html = self.get_raw_html(url).text\n",
    "        return {\n",
    "            \"url\" : url,\n",
    "            \"backlinks\" : self.scrape_all_urls(raw_soup_html),\n",
    "            \"rawText\" : self.scrape_raw_text(raw_soup_html)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b75e8f0b-be9a-4fb6-819c-2b8360334d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped: https://photographylife.com/reviews/fuji-x100f\n"
     ]
    },
    {
     "ename": "MissingSchema",
     "evalue": "Invalid URL 'photographylife.com': No scheme supplied. Perhaps you meant http://photographylife.com?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMissingSchema\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3788\\1685473253.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[0mdepth_limit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m     \u001b[0mscrapeLevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtinderURL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_limit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3788\\1685473253.py\u001b[0m in \u001b[0;36mscrapeLevel\u001b[1;34m(url_set, depth, depth_limit)\u001b[0m\n\u001b[0;32m     62\u001b[0m                 \u001b[0mall_backlink\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdateLink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstarter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Scraped: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstarter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m                 \u001b[0mscrapeLevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_backlink\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_limit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3788\\1685473253.py\u001b[0m in \u001b[0;36mscrapeLevel\u001b[1;34m(url_set, depth, depth_limit)\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlinkChecker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malreadyScrape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstarter\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m                 \u001b[1;31m# scrape and update database and got all backlink\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m                 \u001b[0mall_backlink\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdateLink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstarter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Scraped: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstarter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[0mscrapeLevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_backlink\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth_limit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3788\\1685473253.py\u001b[0m in \u001b[0;36mupdateLink\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mupdateLink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mnewid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataPipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetUniqueID\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mraw_mat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpageScraper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscrape_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mall_backlinks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mraw_mat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"backlinks\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3788\\2151514162.py\u001b[0m in \u001b[0;36mscrape_page\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mscrape_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;34m\"\"\"Return a dictionary of url, all unrepeated backlinks and raw text\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mraw_soup_html\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_raw_html\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         return {\n\u001b[0;32m     48\u001b[0m             \u001b[1;34m\"url\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3788\\2151514162.py\u001b[0m in \u001b[0;36mget_raw_html\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m# webReq = requests.get(url)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m# return requests.get(url, headers=self.headers)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mres_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres_temp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mres_temp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m     \"\"\"\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"get\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    571\u001b[0m             \u001b[0mhooks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m         )\n\u001b[1;32m--> 573\u001b[1;33m         \u001b[0mprep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    574\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m         \u001b[0mproxies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproxies\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mprepare_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    483\u001b[0m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPreparedRequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 484\u001b[1;33m         p.prepare(\n\u001b[0m\u001b[0;32m    485\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m             \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mprepare\u001b[1;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 368\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    369\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_headers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_cookies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcookies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mprepare_url\u001b[1;34m(self, url, params)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mscheme\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m             raise MissingSchema(\n\u001b[0m\u001b[0;32m    440\u001b[0m                 \u001b[1;34mf\"Invalid URL {url!r}: No scheme supplied. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                 \u001b[1;34mf\"Perhaps you meant http://{url}?\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMissingSchema\u001b[0m: Invalid URL 'photographylife.com': No scheme supplied. Perhaps you meant http://photographylife.com?"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "def get_domain(url):\n",
    "    \"\"\"Get domain name (example.com) from a url\"\"\"\n",
    "    parsed_url = urlparse(url)\n",
    "    domain = parsed_url.netloc\n",
    "    if domain.startswith('www.'):\n",
    "        domain = domain[4:]\n",
    "    return domain\n",
    "\n",
    "def word_frequency_dict(words_list):\n",
    "    \"\"\"Turn list of words into dictionary with word as key and frequency as value\"\"\"\n",
    "    frequency_dict = {}\n",
    "    for word in words_list:\n",
    "        if word in frequency_dict:\n",
    "            frequency_dict[word] += 1\n",
    "        else:\n",
    "            frequency_dict[word] = 1\n",
    "    return frequency_dict\n",
    "\n",
    "\n",
    "def updateLink(url):\n",
    "    newid = dataPipeline.getUniqueID()\n",
    "    raw_mat = pageScraper.scrape_page(url)\n",
    "    \n",
    "    all_backlinks = raw_mat[\"backlinks\"]\n",
    "\n",
    "    page_domain = get_domain(url)\n",
    "\n",
    "    all_domain = [] # <<<<<<<<< All domain\n",
    "\n",
    "    for link in all_backlinks:\n",
    "        # get domain of url\n",
    "        domain_temp = get_domain(link)\n",
    "        if (domain_temp not in all_domain):\n",
    "            all_domain.append(domain_temp)\n",
    "            \n",
    "    cleanText_list = TextCleaner.clean(raw_mat[\"rawText\"])\n",
    "    word_dict = word_frequency_dict(cleanText_list)\n",
    "    sorted_word_dict = dict(sorted(word_dict.items()))\n",
    "    \n",
    "    dataPipeline.updateReferenceDomain(all_domain)\n",
    "    dataPipeline.updateWebData(newid, url, sorted_word_dict, all_domain)\n",
    "    dataPipeline.updateInvertedIndexing(newid, sorted_word_dict)\n",
    "    \n",
    "    return all_domain\n",
    "    \n",
    "def removeData(url):\n",
    "    \"\"\"Remove Data By URL\"\"\"\n",
    "    print(\"Removing : \", url)\n",
    "    temp_datarow = dataPipeline.fetch_data_by_url(url)\n",
    "    \n",
    "    dataPipeline.removeWebData(temp_datarow['URL'])\n",
    "    dataPipeline.uncountRef(temp_datarow['Ref_To'])\n",
    "    dataPipeline.removeInvertedIndex(temp_datarow['Web_ID'], temp_datarow['All_Word'])\n",
    "\n",
    "def scrapeLevel(url_set, depth, depth_limit):\n",
    "    if depth <= depth_limit:\n",
    "        for starter in url_set:\n",
    "            if (linkChecker.alreadyScrape(starter) == False):\n",
    "                # scrape and update database and got all backlink\n",
    "                all_backlink = updateLink(starter)\n",
    "                print(\"Scraped: \" + starter)\n",
    "                scrapeLevel(all_backlink, depth+1, depth_limit)\n",
    "\n",
    "            else:\n",
    "                print(\"Already Scraped: \" + starter)\n",
    "                # check if still accessible\n",
    "                if (linkChecker.checkAccessibility(starter) == False):\n",
    "                    removeData(starter)\n",
    "                else:\n",
    "                    pass\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "# main program\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    tinderURL = {\n",
    "        \"https://photographylife.com/reviews/fuji-x100f\"\n",
    "        # \"https://www.dpreview.com/reviews/sony-a7rv-review?utm_source=self-desktop&utm_medium=marquee&utm_campaign=traffic_source\"\n",
    "        # \"https://www.35mmc.com/02/02/2023/hedeco-lime-two-low-profile-shoe-mount-light-meter-review/\",\n",
    "        # \"https://petapixel.com/2023/02/03/canon-usa-settles-with-employees-affected-by-2020-ransomware-attack/\",\n",
    "        # \"https://www.35mmc.com/14/10/2021/pentax-iqzoom-928-review/\"\n",
    "    }\n",
    "\n",
    "    db_path = \"database_3.db\"\n",
    "\n",
    "    TextCleaner = TextCleaners()\n",
    "    pageScraper = pageScrapers()\n",
    "    linkChecker = LinkCheckers(db_path)\n",
    "    dataPipeline = dataPipelines(db_path)\n",
    "    \n",
    "    depth_limit = 2\n",
    "    scrapeLevel(tinderURL, 1, depth_limit)\n",
    "        \n",
    "    \n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c182645-6c44-44f2-b6cb-6f9ef91c9774",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
