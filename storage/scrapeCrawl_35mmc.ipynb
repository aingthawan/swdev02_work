{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d9c2c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A code to scrape name from 35mmc.com\n",
    "# scrape for a defined level\n",
    "# Aing KK.\n",
    "\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import mysql.connector\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7a68057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_level(db, url, level):\n",
    "    # depth limit for crawling url\n",
    "    if level <= 7:\n",
    "        webReq = requests.get(url)\n",
    "        soup_obj = BeautifulSoup(webReq.text, \"html.parser\")\n",
    "        if soup_obj.find('article').text != None:\n",
    "            raw_text = soup_obj.find('article').text\n",
    "            # get title\n",
    "            get_title = soup_obj.find('h1', {'class':'entry-title'}).text\n",
    "            print(\"Current level : \", level, \" at : \", get_title)\n",
    "            # store to db\n",
    "            db.store_db(get_title, url, raw_text)\n",
    "            # time.sleep(0.500)\n",
    "            return [i.find('a')['href'] for i in soup_obj.find_all('div', {'class':'sp-col-4'})]\n",
    "        else:\n",
    "            pass\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f0c93556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_3level(db, url, level):\n",
    "    for link in crawl_level(db, url, level):\n",
    "        crawl_3level(db, link, level+1)\n",
    "    # print(\"level : \", level, \" Completed\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cd5c26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class databasePipeline:\n",
    "    def __init__(self, host, user, password, database, table_name):\n",
    "        print(\"Initialize Database Pipeline\")\n",
    "        self.host = host\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.database = database\n",
    "        self.table_name = table_name\n",
    "        self.create_connection()\n",
    "        self.curr = self.conn.cursor()\n",
    "        self.create_table()\n",
    "\n",
    "    def create_connection(self):\n",
    "        \"\"\"Create a connection to the database\"\"\"\n",
    "        print(\"Creating Connection . . .\")        self.conn = mysql.connector.connect(\n",
    "            host=self.host,\n",
    "            user=self.user,\n",
    "            password=self.password,\n",
    "            database=self.database\n",
    "        )\n",
    "        print(\"Connceted.\")\n",
    "    \n",
    "    def create_table(self):\n",
    "        print(\"Creating table . . .\")\n",
    "        self.curr.execute(\"\"\"DROP TABLE IF EXISTS 35mmc_raw\"\"\")\n",
    "        self.curr.execute(\"\"\"create table 35mmc_raw(\n",
    "            title text,\n",
    "            url text,\n",
    "            raw_text text\n",
    "        )\"\"\")\n",
    "        print(\"Done Table.\")\n",
    "\n",
    "    def store_db(self, title, url, raw_text):\n",
    "        self.curr.execute(\n",
    "            \"\"\"INSERT INTO {} (title, url, raw_text) VALUES (%s, %s, %s)\"\"\".format(self.table_name),\n",
    "            (title, url, raw_text)\n",
    "        )\n",
    "        self.conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5130fd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_url = 'https://www.35mmc.com/20/05/2016/halina-af700-review/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dcaa6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize Database Pipeline\n",
      "Creating Connection . . .\n",
      "Connceted.\n",
      "Creating table . . .\n",
      "Done Table.\n"
     ]
    }
   ],
   "source": [
    "db = databasePipeline('localhost', 'root', 'aingg', '35mmc', '35mmc_raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c92b73-b58b-4ded-9a95-8dac2eb6beaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_level = 1 \n",
    "crawl_3level(db, start_url, start_level)\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57e54c0d-a908-4593-b37c-36355e54e134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# url = 'https://www.35mmc.com/20/05/2016/halina-af700-review/'\n",
    "# url = \"https://fstoppers.com/film/how-was-i-missing-medium-format-point-and-shoot-short-term-review-fujifilm-ga645-470116\"\n",
    "# url = \"https://www.trustedreviews.com/reviews/nikon-z9\"\n",
    "# url = \"https://artyt.me/2022/03/21/%e0%b8%a3%e0%b8%b5%e0%b8%a7%e0%b8%b4%e0%b8%a7-10-%e0%b8%81%e0%b8%a5%e0%b9%89%e0%b8%ad%e0%b8%87%e0%b8%9f%e0%b8%b4%e0%b8%a5%e0%b9%8c%e0%b8%a1%e0%b8%97%e0%b8%b5%e0%b9%88%e0%b8%84%e0%b8%a7%e0%b8%a3/\"\n",
    "url = \" https://artyt.me/tag/%e0%b8%a5%e0%b9%89%e0%b8%b2%e0%b8%87%e0%b8%9f%e0%b8%b4%e0%b8%a5%e0%b9%8c%e0%b8%a1/\"\n",
    "# url = \"https://pantip.com/topic/41822838\"\n",
    "\n",
    "webReq = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2eb60c3a-ad7d-4cb9-85de-022c4de02eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scrape_raw_text(webReq):\n",
    "#     \"\"\"Input responsed requested object (webReq = requests.get(url)), Return as raw text string\"\"\"\n",
    "#     soup_obj = BeautifulSoup(webReq.text, \"html.parser\")\n",
    "#     article = ''\n",
    "#     # Get Header Text\n",
    "#     for raw in soup_obj.find_all('h1'):\n",
    "#         article = article + \" \" + raw.text\n",
    "#     # Get Article text\n",
    "#     for raw in soup_obj.find_all('p'):\n",
    "#         article = article + \" \" + raw.text\n",
    "#     return article\n",
    "\n",
    "def scrape_raw_text(webReq):\n",
    "    \"\"\"Input responsed requested object (webReq = requests.get(url)), Return as raw text string\"\"\"\n",
    "    soup_obj = BeautifulSoup(webReq.text, \"html.parser\")\n",
    "    article = ''\n",
    "    # Get Header Text and Article text\n",
    "    for raw in soup_obj.find_all(['h1', 'p']):\n",
    "        article += \" \" + raw.text\n",
    "    return article\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b689662-7b42-4ebf-b482-38fddecf0329",
   "metadata": {},
   "source": [
    "# Compilation class for scrape url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dc42019c-e806-422d-a8c2-44741483d049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import mysql.connector\n",
    "import time\n",
    "import validators\n",
    "\n",
    "# url = 'https://www.35mmc.com/20/05/2016/halina-af700-review/'\n",
    "# url = \"https://fstoppers.com/film/how-was-i-missing-medium-format-point-and-shoot-short-term-review-fujifilm-ga645-470116\"\n",
    "# url = \"https://www.trustedreviews.com/reviews/nikon-z9\"\n",
    "# url = \"https://artyt.me/2022/03/21/%e0%b8%a3%e0%b8%b5%e0%b8%a7%e0%b8%b4%e0%b8%a7-10-%e0%b8%81%e0%b8%a5%e0%b9%89%e0%b8%ad%e0%b8%87%e0%b8%9f%e0%b8%b4%e0%b8%a5%e0%b9%8c%e0%b8%a1%e0%b8%97%e0%b8%b5%e0%b9%88%e0%b8%84%e0%b8%a7%e0%b8%a3/\"\n",
    "url = \" https://artyt.me/tag/%e0%b8%a5%e0%b9%89%e0%b8%b2%e0%b8%87%e0%b8%9f%e0%b8%b4%e0%b8%a5%e0%b9%8c%e0%b8%a1/\"\n",
    "# url = \"https://pantip.com/topic/41822838\"\n",
    "\n",
    "webReq = requests.get(url)\n",
    "    \n",
    "def scrape_raw_text(webReq):\n",
    "    \"\"\"Input responsed requested object (webReq = requests.get(url)), Return as raw text string\"\"\"\n",
    "    soup_obj = BeautifulSoup(webReq.text, \"html.parser\")\n",
    "    article = ''\n",
    "    # Get Header Text and Article text\n",
    "    for raw in soup_obj.find_all(['h1', 'p']):\n",
    "        article += \" \" + raw.text\n",
    "    return article\n",
    "\n",
    "def scrape_all_link(webReq):\n",
    "    \"\"\"Input responsed requested object (webReq = requests.get(url)), Return as list of all URLs string\"\"\"\n",
    "    soup_obj = BeautifulSoup(webReq.text, \"html.parser\")\n",
    "    return_url = []\n",
    "    for url_obj in soup_obj.find_all('a'):\n",
    "        url = url_obj.get('href')\n",
    "        if url == None:\n",
    "            pass\n",
    "        elif (url.find(\"https:\")==0 or url.find(\"http:\")==0) and (url not in return_url) :\n",
    "            return_url.append(url)   \n",
    "    return return_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2d607945-1809-4314-8db3-6f7366569b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import validators\n",
    "\n",
    "class WebScraper:\n",
    "    def __init__(self, webReq):\n",
    "        self.soup_obj = BeautifulSoup(webReq.text, \"html.parser\")\n",
    "        \n",
    "    def scrape_raw_text(self):\n",
    "        \"\"\"Return raw text string\"\"\"\n",
    "        article = ''\n",
    "        for raw in self.soup_obj.find_all(['h1', 'p']):\n",
    "            article += \" \" + raw.text\n",
    "        return article\n",
    "    \n",
    "    def scrape_all_link(self):\n",
    "        \"\"\"Return list of all URLs string\"\"\"\n",
    "        return_url = []\n",
    "        for url_obj in self.soup_obj.find_all('a'):\n",
    "            url = url_obj.get('href')\n",
    "            if url == None:\n",
    "                pass\n",
    "            elif (url.find(\"https:\")==0 or url.find(\"http:\")==0) and (url not in return_url) :\n",
    "                return_url.append(url)   \n",
    "        return return_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a75b3068-7ed7-4b14-8e00-7fb4ed55923a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://artyt.me/2022/03/21/%e0%b8%a3%e0%b8%b5%e0%b8%a7%e0%b8%b4%e0%b8%a7-10-%e0%b8%81%e0%b8%a5%e0%b9%89%e0%b8%ad%e0%b8%87%e0%b8%9f%e0%b8%b4%e0%b8%a5%e0%b9%8c%e0%b8%a1%e0%b8%97%e0%b8%b5%e0%b9%88%e0%b8%84%e0%b8%a7%e0%b8%a3/\"\n",
    "\n",
    "webReq = requests.get(url)\n",
    "scraper = WebScraper(webReq)\n",
    "article = scraper.scrape_raw_text()\n",
    "links = scraper.scrape_all_link()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b474b4c8-ca87-4845-b895-ec9f7202d237",
   "metadata": {},
   "source": [
    "# Optimized Class For scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "4129b895-18e6-4b97-9553-edcdeb01e50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import validators\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "class pageScraper:\n",
    "    \"\"\"Class for Scrape single page, Return URL, all backlinks and Raw Text\"\"\"\n",
    "    def __init__(self):\n",
    "        # Set allowed domain\n",
    "        self.allowed_domain = [\n",
    "            \"artyt.me\",\n",
    "            \"www.35mmc.com\",\n",
    "            \"www.dpreview.com\"\n",
    "        ]\n",
    "    \n",
    "    def get_raw_html(self, url):\n",
    "        \"\"\"get raw html soup obj\"\"\"\n",
    "        # webReq = requests.get(url)\n",
    "        return BeautifulSoup((requests.get(url)).text, \"html.parser\")\n",
    "    \n",
    "    def scrape_raw_text(self, soup_obj):\n",
    "        \"\"\"Return raw text string from bs4 boject\"\"\"\n",
    "        return ' '.join([raw.text for raw in soup_obj.find_all(['h1', 'p'])])\n",
    "    \n",
    "    def scrape_all_urls(self, soup_obj):\n",
    "        \"\"\"Return list of all URLs string from bs4 object\"\"\"\n",
    "        return_url = []\n",
    "        for url_obj in soup_obj.find_all('a'):\n",
    "            url = url_obj.get('href')\n",
    "            if (url is not None) and validators.url(url) and (url not in return_url) and ((urlparse(url).netloc) in self.allowed_domain):\n",
    "                return_url.append(url)\n",
    "                # print(url)\n",
    "                # print(urlparse(url).netloc)\n",
    "        return return_url\n",
    "    \n",
    "    def scrape_page(self, url):\n",
    "        \"\"\"Return a dictionary of url and raw text\"\"\"\n",
    "        raw_soup_html = self.get_raw_html(url)\n",
    "        return {\n",
    "            \"url\" : url,\n",
    "            \"backlinks\" : self.scrape_all_urls(raw_soup_html),\n",
    "            \"rawText\" : self.scrape_raw_text(raw_soup_html)\n",
    "        }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "86e89d01-d4fa-470c-9e84-a7402d124407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://www.example.com/',\n",
       " 'backlinks': [],\n",
       " 'rawText': 'Example Domain This domain is for use in illustrative examples in documents. You may use this\\n    domain in literature without prior coordination or asking for permission. More information...'}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = pageScraper()\n",
    "ps.scrape_page(\"https://www.example.com/\")\n",
    "# ps.scrape_page(\"https://www.example.com/\")[\"url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a919571f-c870-44db-9089-6e28b043dccc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
