{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd22e79e-f08c-4f3d-b65b-ae1f2dc53adb",
   "metadata": {},
   "source": [
    "# Inverted Index Search from Indexed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47063e8c-f794-4a8f-8c93-cfb4d07a01ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Read the csv file\n",
    "# invertedIndex file\n",
    "# OneDrive/Documents/GitHub/swdev02_work/project/temp_src/invertedIndex_v0.csv\n",
    "invertedIndex = pd.read_csv('csv/invertedIndex_v1.csv', index_col=0)\n",
    "# webData file\n",
    "webData = pd.read_csv('csv/webData_v0.csv', index_col=0)\n",
    "\n",
    "class TextCleaner:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def normalize(self, raw_text):\n",
    "        \"\"\"Remove special characters and lowercase text\"\"\"\n",
    "        return re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \" \", raw_text.lower())\n",
    "\n",
    "    def remove_stopwords(self, raw_text):\n",
    "        \"\"\"Remove stopwords\"\"\"\n",
    "        words = [word for word in raw_text.split() if word not in self.stop_words]\n",
    "        return \" \".join(words)\n",
    "\n",
    "    def lemmatize(self, raw_text):\n",
    "        \"\"\"Perform lemmatization\"\"\"\n",
    "        doc = self.nlp(raw_text)\n",
    "        return [token.lemma_ for token in doc]\n",
    "\n",
    "    def clean(self, raw_text):\n",
    "        \"\"\"Clean text by normalizing, removing stopwords, and lemmatizing\"\"\"\n",
    "        raw_text = self.normalize(raw_text)\n",
    "        raw_text = self.remove_stopwords(raw_text)\n",
    "        return self.lemmatize(raw_text)\n",
    "\n",
    "\n",
    "def get_common_id(lists):\n",
    "    # Initialize an empty set to store the common elements\n",
    "    common_data = set(lists[0])\n",
    "    # Iterate over the rest of the lists\n",
    "    for lst in lists[1:]:\n",
    "        # Update the set with the common elements of the current list and the set\n",
    "        common_data.intersection_update(set(lst))\n",
    "    return list(common_data)\n",
    "\n",
    "def return_url_by_id(id):    \n",
    "    try:\n",
    "        return webData.loc[webData['ID'] == id, 'URL'][0]\n",
    "    except KeyError:\n",
    "        return 'No ID found in webData'\n",
    "\n",
    "# Input Query convert to Token\n",
    "def query_clean(input_str):\n",
    "    if (input_str != \"\") and (type(input_str) == str):\n",
    "        text_cleaner = TextCleaner()\n",
    "        normalized_text = text_cleaner.normalize(input_str)\n",
    "        no_stopwords_text = text_cleaner.remove_stopwords(normalized_text)\n",
    "        lemmatized_text = text_cleaner.lemmatize(no_stopwords_text)\n",
    "        return lemmatized_text\n",
    "    else:\n",
    "        return 'Input Error'\n",
    "\n",
    "# Token to ID list\n",
    "def token_to_match_list_list(token_list):\n",
    "    res_temp = []\n",
    "    for token in token_list:\n",
    "        # get dict of each token and make a list\n",
    "        res_temp.append( ast.literal_eval(invertedIndex.loc[invertedIndex['Gram'] == token, 'DocsID_Dict'][0]) )\n",
    "    return res_temp\n",
    "\n",
    "def search_inverted_index(inputQuery):\n",
    "    output_temp = get_common_id(token_to_match_list_list(query_clean(inputQuery)))\n",
    "    print('Total ', len(output_temp), \" Results\")\n",
    "    for id in output_temp:\n",
    "        print(return_url_by_id(id))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0444760f-5dae-4d22-9a6a-fe8cbe99193e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Search : \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "None pentax medium format\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching :  pentax medium format\n",
      "Total  13  Results\n",
      "https://www.35mmc.com/25/11/2017/rollei-35-cameras-review/\n",
      "https://www.35mmc.com/11/01/2019/minox-35-gt-guest-review/\n",
      "https://www.35mmc.com/11/01/2019/minox-35-gt-guest-review/\n",
      "https://www.35mmc.com/08/11/2020/my-journey-to-infrared-photography-by-markus-hofstatter/\n",
      "https://www.35mmc.com/03/06/2021/my-journey-shooting-85-analog-cameras-and-writing-a-book-about-it-by-christof-bircher/\n",
      "https://www.35mmc.com/03/06/2021/my-journey-shooting-85-analog-cameras-and-writing-a-book-about-it-by-christof-bircher/\n",
      "https://www.35mmc.com/01/07/2017/making-sony-a7rii-work/\n",
      "https://www.35mmc.com/11/01/2019/minox-35-gt-guest-review/\n",
      "https://www.35mmc.com/08/11/2020/my-journey-to-infrared-photography-by-markus-hofstatter/\n",
      "https://www.35mmc.com/31/05/2019/minolta-hi-matic-7-review/\n",
      "https://www.35mmc.com/22/08/2021/montoggio-a-chronicle-of-an-absence-by-salvatore-da-cha/\n",
      "https://www.35mmc.com/02/05/2022/the-descent-of-a-film-photographer-part-1-from-colour-to-black-white-by-nandakumar/\n",
      "https://www.35mmc.com/15/11/2019/fuji-natura-1600-shooting-star-trails-and-at-night/\n",
      "Input Search : \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "None !ex\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching :  !ex\n",
      "\n",
      "EXIT . . .\n"
     ]
    }
   ],
   "source": [
    "inputQuery = \"\"\n",
    "\n",
    "while inputQuery != \"!ex\":\n",
    "    inputQuery = input(print(\"Input Search : \"))\n",
    "    print(\"Searching : \", inputQuery)\n",
    "    if inputQuery == \"!ex\":\n",
    "        pass\n",
    "    else:\n",
    "        search_inverted_index(inputQuery)    \n",
    "    \n",
    "print(\"\\nEXIT . . .\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc08ce75-5a45-4779-86ef-e7a26ebf42b3",
   "metadata": {},
   "source": [
    "# Class form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a01a4b2-308b-4fb8-9563-d99b36e029d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "class TextCleaner:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def normalize(self, raw_text):\n",
    "        \"\"\"Remove special characters and lowercase text\"\"\"\n",
    "        return re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \" \", raw_text.lower())\n",
    "\n",
    "    def remove_stopwords(self, raw_text):\n",
    "        \"\"\"Remove stopwords\"\"\"\n",
    "        words = [word for word in raw_text.split() if word not in self.stop_words]\n",
    "        return \" \".join(words)\n",
    "\n",
    "    def lemmatize(self, raw_text):\n",
    "        \"\"\"Perform lemmatization\"\"\"\n",
    "        doc = self.nlp(raw_text)\n",
    "        return [token.lemma_ for token in doc]\n",
    "\n",
    "    def clean(self, raw_text):\n",
    "        \"\"\"Clean text by normalizing, removing stopwords, and lemmatizing\"\"\"\n",
    "        raw_text = self.normalize(raw_text)\n",
    "        raw_text = self.remove_stopwords(raw_text)\n",
    "        return self.lemmatize(raw_text)\n",
    "\n",
    "\n",
    "class InvertedIndexSearch:\n",
    "    def __init__(self):\n",
    "        self.invertedIndex = pd.read_csv('csv/invertedIndex_v1.csv', index_col=0)\n",
    "        self.webData = pd.read_csv('csv/webData_v0.csv', index_col=0)\n",
    "    \n",
    "    def get_common_id(self, lists):\n",
    "        common_data = set(lists[0])\n",
    "        for lst in lists[1:]:\n",
    "            common_data.intersection_update(set(lst))\n",
    "        return list(common_data)\n",
    "\n",
    "    def return_url_by_id(self, id):    \n",
    "        try:\n",
    "            return self.webData.loc[self.webData['ID'] == id, 'URL'][0]\n",
    "        except KeyError:\n",
    "            return 'No ID found in webData'\n",
    "\n",
    "    def query_clean(self, input_str):\n",
    "        if (input_str != \"\") and (type(input_str) == str):\n",
    "            text_cleaner = TextCleaner()\n",
    "            normalized_text = text_cleaner.normalize(input_str)\n",
    "            no_stopwords_text = text_cleaner.remove_stopwords(normalized_text)\n",
    "            lemmatized_text = text_cleaner.lemmatize(no_stopwords_text)\n",
    "            return lemmatized_text\n",
    "        else:\n",
    "            return 'Input Error'\n",
    "\n",
    "    def token_to_match_list_list(self, token_list):\n",
    "        res_temp = []\n",
    "        for token in token_list:\n",
    "            res_temp.append( ast.literal_eval(self.invertedIndex.loc[self.invertedIndex['Gram'] == token, 'DocsID_Dict'][0]) )\n",
    "        return res_temp\n",
    "\n",
    "    def search_inverted_index(self, inputQuery):\n",
    "        inp_query_list = self.query_clean(inputQuery)\n",
    "        output_temp = self.get_common_id(self.token_to_match_list_list(inp_query_list))\n",
    "        # print('Total ', len(output_temp), \" Results\")\n",
    "        # for id in output_temp:\n",
    "        #     # return as URLs\n",
    "        #     # print(self.return_url_by_id(id))\n",
    "        #     # return as ID\n",
    "        #     print(id)\n",
    "        return output_temp\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19f3ea2a-de52-4fb3-af91-32ead67cd56c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[903,\n",
       " 135,\n",
       " 525,\n",
       " 142,\n",
       " 271,\n",
       " 1040,\n",
       " 147,\n",
       " 916,\n",
       " 1049,\n",
       " 538,\n",
       " 1058,\n",
       " 930,\n",
       " 293,\n",
       " 296,\n",
       " 427,\n",
       " 301,\n",
       " 48,\n",
       " 565,\n",
       " 569,\n",
       " 186,\n",
       " 961,\n",
       " 450,\n",
       " 329,\n",
       " 458,\n",
       " 203,\n",
       " 716,\n",
       " 78,\n",
       " 206,\n",
       " 334,\n",
       " 83,\n",
       " 339,\n",
       " 218,\n",
       " 604,\n",
       " 992,\n",
       " 232,\n",
       " 492,\n",
       " 365,\n",
       " 111,\n",
       " 624,\n",
       " 628,\n",
       " 630,\n",
       " 120,\n",
       " 250,\n",
       " 124,\n",
       " 638,\n",
       " 383]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputQuery = \"leica point and shoot film rangefinder camera street photography\"\n",
    "\n",
    "inverted_index = InvertedIndexSearch()\n",
    "# got list of filtered document containing keywords\n",
    "docs_with_words = inverted_index.search_inverted_index(inputQuery)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec356c2e-0f61-40be-a543-f0233152bd0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "29779eaeaa662f518a7c5365664c9c0d0a52b94abeecfec7b0ccc4fc94e18524"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
